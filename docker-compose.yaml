services:
  api:
    build:
      context: .
    environment:
      - DATABASE_URL=${DATABASE_URL:-postgres://self:hosted@db:5432/toi}
      - TOI_CONFIG_PATH=${TOI_CONFIG_PATH:-./toi.json}
    ports:
      - "6969:6969"
    volumes:
      - ./toi_server/toi.json:/usr/app/toi.json
    depends_on:
      db:
        condition: service_healthy

  db:
    image: pgvector/pgvector:${POSTGRES_IAMGE_VERSION:-pg17}
    environment:
      - POSTGRES_USER=${POSTGRES_USER:-self}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-hosted}
      - POSTGRES_DB=${POSTGRES_DB:-toi}
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "sh -c 'pg_isready -U ${POSTGRES_USER:-self} -d ${POSTGRES_DB:-toi}'"]
      interval: 10s
      timeout: 5s
      retries: 5

  embedding:
    image: vllm/vllm-openai:${EMBEDDING_IMAGE_VERSION:-latest}
    command: --model ${EMBEDDING_MODEL_ID:-Snowflake/snowflake-arctic-embed-l-v2.0} --dtype ${GENERATION_DTYPE:-auto} --gpu-memory-utilization ${GENERATION_MEM:-0.2}
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${EMBEDDING_NUM_GPUS:-all}
              capabilities: [gpu]
    profiles:
      - models

  generation:
    image: vllm/vllm-openai:${GENERATION_IMAGE_VERSION:-latest}
    command: --model ${GENERATION_MODEL_ID:-Qwen/Qwen2.5-1.5B-Instruct} --dtype ${GENERATION_DTYPE:-half} --gpu-memory-utilization ${GENERATION_MEM:-0.7} --max-model-len ${GENERATION_LEN:-16000}
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${GENERATION_NUM_GPUS:-all}
              capabilities: [gpu]
    profiles:
      - models
